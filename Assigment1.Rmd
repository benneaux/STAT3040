---
title: "STAT3040 Assignment 1"
author: "Benjamin G. Moran, c3076448@uon.edu.au"
date: 
output:
  pdf_document: default
  html_document:
    includes:
      before_body: eqnnumber.js
    mathjax: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
**Due:** 5pm Tuesday 9th August 2016.


***
1. In general, if two random variables are **uncorrelated**, they **may** or **may not** be **independent**.

    Show that if two random variables $X \sim N[\mu_X, \sigma^2_X]$ and $Y \sim N[\mu_Y, \sigma^2_Y]$ are **uncorrelated**, then they are **independent** as well.
    
    **Answer:** If two random variables $X$ and $Y$ are uncorrelated, then the following is true:
$$\rho_{XY}(X,Y) = \frac{COV(X,Y)}{\sigma_X\sigma_Y} = 0$$.
The joint distribution of two Normal random variables $X$ and $Y$ is given by: 
$$f_{XY}(x,y)=\frac{1}{2 \pi \sigma_X \sigma_Y \sqrt{1-\rho^2}}
\exp\left\{-\frac{1}{2(1-\rho^2)} \left( \frac{(x-\mu_X)^2}{\sigma_X^2} - \frac{2\rho(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y} + \frac{(y-\mu_Y)^2}{\sigma^2_Y}
\right) \right\} $$
If we sub in $\rho = 0$ into this distribution, we get the following:
$$\begin{align} f_{XY}(x,y)&=\frac{1}{2 \pi \sigma_X \sigma_Y}
\exp\left\{-\frac{1}{2} \left( \frac{(x-\mu_X)^2}{\sigma_X^2} + \frac{(y-\mu_Y)^2}{\sigma^2_Y}
\right) \right\} \\ &=\frac{1}{\sqrt{2 \pi \sigma^2_X}}
\exp\left\{-\frac{1}{2} \left( \frac{(x-\mu_X)^2}{\sigma_X^2}\right) \right\} +\frac{1}{\sqrt{2 \pi \sigma^2_Y}}\exp\left\{-\frac{1}{2} \left( \frac{(y-\mu_Y)^2}{\sigma_Y^2}\right) \right\} \\
 &= f_X(x)f_Y(y)
 \end{align}$$
 This shows that the joint pdf of $X$ and $Y$ - $f_{XY}(x,y)$ - is equal to the product of the marginal pdfs of $X$ and $Y$ - $f_X(x)f_Y(y)$. Therefore, the two variables are independent.

***

2. Show that the **joint pdf** of a **Multivariate Normal** distribution with $n = 2$ can be simplified to the **joint pdf** of a **Bivariate Normal** distribution.

    **Answer:** The joint pdf of a Multivariate Normal distribution is given by:
    $$f_X(x)=\frac{1}{\sqrt{(2\pi)^n|\boldsymbol\Gamma|}}\exp\left(-\frac{1}{2}({x}-{\mu_X})^T{\boldsymbol\Gamma}^{-1}({x}-{\mu_X})
\right),$$
where $X:= (X_1, X_2, ..., X_n)'$, $x:=(x_1, x_2, ..., x_n)'$ and $\mu:= (\mu_1, \mu_2, ..., \mu_n)'$. When $n=2$, the Covariance Matrix $\Gamma$ is given by:
$$\begin{align} \Gamma &= \begin{bmatrix} \sigma^2_{X_1} & \rho\sigma_{X_1}\sigma_{X_2} \\ \rho\sigma_{X_1}\sigma_{X_2} &  \sigma^2_{X_2} \end{bmatrix} \\
\implies \Gamma^{-1} &= \frac{1}{\sigma^2_{X_1}\sigma^2_{X_2}(1-\rho^2)} \begin{bmatrix} \sigma^2_{X_2} & -\rho\sigma_{X_1}\sigma_{X_2} \\ -\rho\sigma_{X_1}\sigma_{X_2} &  \sigma^2_{X_1} \end{bmatrix} \\
&= \frac{1}{(1-\rho^2)}\begin{bmatrix} \frac{1}{\sigma^2_{X_1}} & -\frac{\rho}{\sigma_{X_1}\sigma_{X_2}} \\ -\frac{\rho}{\sigma_{X_1}\sigma_{X_2}} &  \frac{1}{\sigma^2_{X_2}} \end{bmatrix}\\
\implies det(\Gamma) &= \sigma^2_{X_1}\sigma^2_{X_2}(1-\rho^2)
\end{align}$$
If we sub these values into the joint pdf of the Multivariate Normal distribution with $n=2$, we get:
$$\begin{align} f_X(x) &=\frac{1}{2\pi \sqrt{\sigma^2_{X_1}\sigma^2_{X_2}(1-\rho^2)}} \\ & \times \exp \left\{- \frac{1} {2(1-\rho^2)} \left( \begin{pmatrix} x_1 - \mu_{X_1} \\ x_2 - \mu_{X_2} \end{pmatrix}^T  \begin{bmatrix} \frac{1}{\sigma^2_{X_1}} & -\frac{\rho}{\sigma_{X_1}\sigma_{X_2}} \\ -\frac{\rho}{\sigma_{X_1}\sigma_{X_2}} &  \frac{1}{\sigma^2_{X_2}} \end{bmatrix} \begin{pmatrix} x_1 - \mu_{X_1} \\ x_2 - \mu_{X_2} \end{pmatrix}\right) \right\} \\ &= \frac{1}{2\pi\sigma_{X_1}\sigma_{X_2}\sqrt{1-\rho^2}}\exp\left\{-\frac{1}{2(1-\rho^2)} \left( \frac{(x_1-\mu_{X_2})^2}{\sigma_{X_1}^2} - \frac{2\rho(x_1-\mu_{X_1})(x_2-\mu_{X_2})}{\sigma_{X_1}\sigma_{X_2}} + \frac{(x_2-\mu_{X_2})^2}{\sigma^2_{X_2}}\right) \right\} \end{align},$$
which is equivalent to the bivariate Normal Distribution stated in Question 1.

3. **Bonus Question** Consider to random variables $X$ and $Y$ with a completely **linear** relationship, that is $$Y = aX + b,$$ for constants $a,b \in \mathbb R$ and $a \neq 0$. Show that if $a>0$, then $\rho(X,Y) = 1$ and if $a<0$, then $\rho(X,Y) = -1$.

    **Answer**. We know that $\rho(X,Y) = \frac{COV(X,Y)}{\sigma_X\sigma_Y}$ and that $COV(X,Y) = E[XY] - E[X]E[Y]$. Now let's determine every value for $Y$ in terms of $X$ via the linear relationship given in the question.
    $$\begin{align} E[Y] &= E[aX +b] = aE[X] + b \\ \implies E[X]E[Y] &= E[X](aE[X] + b) = aE[X]^2 + bE[X] \\ E[XY] &= E[X(aX + b)] = E[aX^2 + bX] = aE[X^2] + bE[X] \\ Var(Y) &= E[Y^2] - E[Y]^2 = E[(aX + b)^2] - E[aX + b]^2 \\ &= a^2E[X^2]+2abE[X] + b^2 - \left(aE[X] + b\right)^2 \\ &= a^2E[X^2]+2abE[X] + b^2 - a^2E[X]^2 - 2abE[X] - b^2 \\ &= a^2\sigma^2_X \\ \implies \sigma_Y &= \|a\sigma_X\| \end{align}$$ Now we can rewrite the covariance term as: $$\begin{align} COV(X,Y) &= aE[X^2] + bE[X] - \left(aE[X]^2 + bE[X]\right) \\ &= aE[X^2] + bE[X] - aE[X]^2 - bE[X] \\ &= aE[X^2] - aE[X]^2 \\ &= a\sigma^2_X\end{align}$$
    Now, if we sub both of these values into the Correlation equation given earlier, we get:
    $$\begin{align} \rho(X,Y) &= \frac{a\sigma^2_X}{\|a\sigma_X^2\|} \\ &= \frac{a}{\|a\|} \end{align},$$
    which, when $a>0$ will result in $\rho(X,Y) = 1$ but when $a<0$, $\rho(X,Y) = -1$.
    