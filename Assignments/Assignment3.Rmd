---
title: "Assignment3"
author: "Benjamin Moran"
date: "30 August 2016"
output: html_document
header-includes: \usepackage{amsmath} \usepackage{mathtools}
---

###Question 1
Let us consider the $n \times 1$ **dependent vector** $\mathbf{X}$ and the $n \times k$ **observed matrix of independent variables** $Z$ as defined in Slide 10. We construct two **regression models** between $\mathbf{X}$ and $Z$ as follows:

$$
  \begin{cases}
  \begin{aligned}
    (i) \quad \mathbf{X} &= Z\beta_{1} + \pmb{\mathcal{W}}_{1} \\
    (ii) \quad \mathbf{X} &= Z\beta_{2} + \pmb{\mathcal{W}}_{2}
  \end{aligned}
  \end{cases}
$$

where $\pmb{\mathcal{W}}_{1} = (\mathcal{W}_{11}, \cdots, \mathcal{W}_{n1})'$ and $\pmb{\mathcal{W}}_{2} = (\mathcal{W}_{12}, \cdots, \mathcal{W}_{n2})'$, $\mathcal{W}_{11}, \cdots, \mathcal{W}_{n1} \overset{iid}{\sim} N[0,\sigma_{1}^{2}]$  and $\mathcal{W}_{12}, \cdots, \mathcal{W}_{n2} \overset{iid}{\sim} N[0,\sigma_{2}^{2}]$ are two **independent** series. If we define $\pmb{\theta}_{1}:=(\beta_{1}', \sigma_{1}^{2})'$ and $\pmb{\theta}_{2}:=(\beta_{2}', \sigma_{2}^{2})'$, show that the **Kullback-Leibler Divergence** between the joint pdf of $\mathbf{X}$ based on models *(i)* and *(ii)* is given as:

$$
  I(\pmb{\theta}_{1}; \pmb{\theta}_{2}) = \frac{1}{2}\left(\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}} - log\left(\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\right) - 1\right) + \frac{(\beta_{1} - \beta_{2})'Z'Z(\beta_{1} - \beta_{2})}{2n\sigma_{2}^{2}}
$$

**Answer:**

***

###Bonus Question

If the **true** value of the parameter vector is $\pmb{\theta} = (\beta', \sigma^{2})'$ and the **estimated** value based on the **sample** $\widehat{\pmb{\theta}} = (\widehat{\beta'}, \widehat{\sigma^{2}})'$, one may argue that the **best** model would be one that **minimizes** the **Kullback-Leibler distance** between the joint-pdfs of **theoretical** value and the **sample** estimation, say $I(\pmb{\theta}; \widehat{\pmb{\theta}})$. Because $\pmb{\theta}$ will not be known, Hurvich and Tsai (1989) considered finding an **unbiased estimator** for $E_{1}[I(\beta_{1}, \sigma_{1}^{2}; \widehat{\beta}, \widehat{\sigma}^{2})]$, where

$$
  I(\beta, \sigma^{2}; \widehat{\beta}, \widehat{\sigma}^{2}) = \frac{1}{2}\left(\frac{\sigma^{2}}{\widehat{\sigma}^{2}} - log\left(\frac{\sigma^{2}}{\widehat{\sigma}^{2}}\right) - 1\right) + \frac{(\beta - \widehat{\beta})'Z'Z(\beta - \widehat{\beta})}{2n\widehat{\sigma}^{2}}
$$

and $\beta$ is a $k \times 1$ regression parameter vector. Show that

$$
  E_{\pmb{\theta}}[I(\beta_{1}, \sigma_{1}^{2}; \widehat{\beta}, \widehat{\sigma}^{2})] = \frac{1}{2}\left(-log(\sigma_{1}^{2}) + E_{1}[log(\widehat{\sigma}^{2})] + \frac{n+k}{n-k-2} - 1\right).
$$

**Answer:** Expectation is a linear function, so we can rewrite the above as 
$$
E_{\pmb{\theta}}I(\beta, \sigma^{2}; \widehat{\beta}, \widehat{\sigma}^{2}) = \frac{1}{2}E\left[\frac{\sigma^{2}}{\widehat{\sigma}^{2}} - log\left(\frac{\sigma^{2}}{\widehat{\sigma}^{2}}\right) - 1 + \frac{(\beta - \widehat{\beta})'Z'Z(\beta - \widehat{\beta})}{n\widehat{\sigma}^{2}}\right]
$$

From the reference text, we are given that:
$$
\begin{aligned}
  \frac{n\widehat{\sigma}^{2}}{\sigma_{1}^{2}} &\sim \chi_{n-k}^{2} \\
  \frac{(\widehat{\beta} - \beta)'Z'Z(\widehat{\beta} - \beta)}{n\widehat{\sigma}^{2}} &\sim \chi_{k}^{2}
\end{aligned}
$$
We are also given that if $x \sim \chi_{n}^{2} \implies E[(\frac{1}{x})] = \frac{1}{n-2}$. So:
$$
\begin{aligned}
 \frac{n\widehat{\sigma}^{2}}{\sigma_{1}^{2}} &= n\left(\frac{\widehat{\sigma}^{2}}{\sigma_{1}^{2}}\right)\\
  &=n\left(\frac{\sigma_{1}^{2}}{\widehat{\sigma}^{2}}\right)^{-1} \\
  \implies E\left[n\left(\frac{\widehat{\sigma}^{2}}{\sigma_{1}^{2}}\right)\right] &= n*\left(\frac{1}{(n-k)-2}\right) \\
  &=\frac{n}{n-k-2}
\end{aligned}
$$

Taking the expectation of a scalar returns the scalar. So we can simplify the above as:
$$
  E_{\pmb{\theta}}I(\beta, \sigma^{2}; \widehat{\beta}, \widehat{\sigma}^{2}) = \frac{1}{2}E\left[\frac{\sigma^{2}}{\widehat{\sigma}^{2}} - log\left(\frac{\sigma^{2}}{\widehat{\sigma}^{2}}\right) - 1 + \frac{(\beta - \widehat{\beta})'Z'Z(\beta - \widehat{\beta})}{n\widehat{\sigma}^{2}}\right]
$$


ch ch ch ch changes
***
