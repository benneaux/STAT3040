---
title: "Assignment3"
author: "Benjamin Moran"
date: "30 August 2016"
output: html_document

---
Given the random vector $\pmb{Y}$, the **Kullback-Leibler Divergence** is the **distance**
between two joint pdfs in the same family, indexed by a **parameter** $\pmb{\theta}$, say
$f_{\pmb{Y}}(\pmb{y}; \pmb{\theta}_{1})$ and $f_{\pmb{Y}}(\pmb{y}; \pmb{\theta}_{2})$, defined as

$$I(\pmb{\theta}_{1};\pmb{\theta}_{2}) = \frac{1}{n}E_{1}\left[log\left(\frac{f_{\pmb{Y}}(\pmb{Y}; \pmb{\theta}_{1})}{f_{\pmb{Y}}(\pmb{Y};\pmb{\theta}_{2})} \right) \right]$$

where $E_{1}$ denotes **expectation** with respect to the **joint pdf** determined by $\pmb{\theta}_{1}$.
That is,

$$I(\pmb{\theta}_{1};\pmb{\theta}_{2}) = \frac{1}{n} \int\limits_{-\infty}^{\infty} \cdots \int\limits_{-\infty}^{\infty} log\left(\frac{f_{\pmb{Y}}(\pmb{Y}; \pmb{\theta}_{1})}{f_{\pmb{Y}}(\pmb{Y};\pmb{\theta}_{2})} \right)f_{\pmb{Y}}(\pmb{y};\pmb{\theta}_{1})d\pmb{y}$$

###Question 1
Let us consider the $n \times 1$ **dependent vector** $\mathbf{X}$ and the $n \times k$ **observed matrix of independent variables** $Z$ as defined in Slide 10. We construct two **regression models** between $\mathbf{X}$ and $Z$ as follows:

$$
  \begin{cases}
  \begin{aligned}
    (i) \quad \mathbf{X} &= Z\beta_{1} + \pmb{\mathcal{W}}_{1} \\
    (ii) \quad \mathbf{X} &= Z\beta_{2} + \pmb{\mathcal{W}}_{2}
  \end{aligned}
  \end{cases}
$$

where $\pmb{\mathcal{W}}_{1} = (\mathcal{W}_{11}, \cdots, \mathcal{W}_{n1})'$ and $\pmb{\mathcal{W}}_{2} = (\mathcal{W}_{12}, \cdots, \mathcal{W}_{n2})'$, $\mathcal{W}_{11}, \cdots, \mathcal{W}_{n1} \overset{iid}{\sim} N[0,\sigma_{1}^{2}]$  and $\mathcal{W}_{12}, \cdots, \mathcal{W}_{n2} \overset{iid}{\sim} N[0,\sigma_{2}^{2}]$ are two **independent** series. If we define $\pmb{\theta}_{1}:=(\beta_{1}', \sigma_{1}^{2})'$ and $\pmb{\theta}_{2}:=(\beta_{2}', \sigma_{2}^{2})'$, show that the **Kullback-Leibler Divergence** between the joint pdf of $\mathbf{X}$ based on models *(i)* and *(ii)* is given as:

$$
  I(\pmb{\theta}_{1}; \pmb{\theta}_{2}) = \frac{1}{2}\left(\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}} - log\left(\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\right) - 1\right) + \frac{(\beta_{1} - \beta_{2})'Z'Z(\beta_{1} - \beta_{2})}{2n\sigma_{2}^{2}}
$$

**Answer:** We are given (in the lecture notes) that the random observation vector $\pmb{X}$ - corresponding to the Regression Model defined in the question - has a **Multivariate Normal** distribution with **mean** vector $Z\beta$ and the **Covariance** matrix $\sigma_{\mathcal{W}}^{2}I = \Gamma$, where $I$ is an $n \times n$ identity matrix (Chapter 2: slide 14). The pdf for a Mutivariate Normal Distribution is given by:

  $$
    f_{\pmb{X}}(\pmb{X};\pmb{\theta})=
    (2\pi)^{-\frac{n}{2}}|\Gamma|^{-\frac{1}{2}}
    \exp\left\{
      -\frac{1}{2}({x}-{Z\beta})^T\left[{\Gamma}\right]^{-1}({x}-{Z\beta})
    \right\}
  $$

We can rewrite the expression for the **Kullback-Leibler Divergence** above as:
  
  $$
    \begin{aligned}
      I(\pmb{\theta}_{1}; \pmb{\theta}_{2}) 
      &= \int\limits_{\mathbb{R}^{2}} 
        p_{\pmb{X}}(\pmb{X};\pmb{\theta}_{1}) 
        log
          \left(
            \frac
              {p_{\pmb{X}}(\pmb{X};\pmb{\theta}_{1})}
              {q_{\pmb{X}}(\pmb{X};\pmb{\theta}_{2})}
          \right) \\
      &= \int\limits_{\mathbb{R}^{2}}p_{\pmb{X}}(\pmb{X};\pmb{\theta}_{1}) \, 
          log
            \left(
              \frac
                {(2\pi)^{-\frac{n}{2}}|\Gamma_{1}|^{-\frac{1}{2}}
                  \exp
                    \left\{
                      -\frac
                        {1}{2}
                      ({x}-{Z\beta_{1}})^T\Gamma_{1}^{-1}({x}-{Z\beta_{1}})
                    \right\}}
                {(2\pi)^{-\frac{n}{2}}|\Gamma_{2}|^{-\frac{1}{2}}
                  \exp
                    \left\{
                      -\frac
                        {1}{2}
                      ({x}-{Z\beta_{2}})^T \Gamma_{2}^{-1} ({x}-{Z\beta_{2}})
                    \right\}}
            \right) \\
      &= -\frac
            {1}{2}
          \int\limits_{\mathbb{R}^{2}}p_{\pmb{X}}(\pmb{X};\pmb{\theta}_{1}) \, 
          log
            \left(
              \frac
                {|\Gamma_{1}|}{|\Gamma_{2}|}
            \right)  
          + \frac
              {1}{2}
            \int\limits_{\mathbb{R}^{2}}p_{\pmb{X}}(\pmb{X};\pmb{\theta}_{1}) \,
            ({x}-{Z\beta_{2}})^T \Gamma_{2}^{-1} ({x}-{Z\beta_{2}})
          - \frac
              {1}{2}
            \int\limits_{\mathbb{R}^{2}}p_{\pmb{X}}(\pmb{X};\pmb{\theta}_{1}) \,
            ({x}-{Z\beta_{1}})^T \Gamma_{1}^{-1} ({x}-{Z\beta_{1}}) \\
      &= -\frac
            {1}{2}
          log
            \left(
              \frac
                {\sigma_{1}^{2}}{\sigma_{2}^{2}}
            \right)  
          + \frac
              {1}{2}
            \int\limits_{\mathbb{R}^{2}}p_{\pmb{X}}(\pmb{X};\pmb{\theta}_{1}) \,
            ({x}-{Z\beta_{2}})^T \Gamma_{2}^{-1} ({x}-{Z\beta_{2}})
          - \frac
              {1}{2}
            \int\limits_{\mathbb{R}^{2}}p_{\pmb{X}}(\pmb{X};\pmb{\theta}_{1}) \,
            ({x}-{Z\beta_{1}})^T \Gamma_{1}^{-1} ({x}-{Z\beta_{1}})            
    \end{aligned}$$





  

  
  Now, if we have two models *(i)* and *(ii)* for $\pmb{X}$ with the parameters stated above, the KL Divergence is given by
  $$
    \begin{aligned}
      I(\pmb{\theta}_{1}; \pmb{\theta}_{2})
      &=\frac{1}{2}E_{1}
        \left[
          log
          \left(
            \frac
              {f_{\pmb{X}}(\pmb{X};\pmb{\theta}_{1})}
              {f_{\pmb{X}}(\pmb{X};\pmb{\theta}_{2})}
          \right)
        \right] \\
      &=\frac{1}{2}E_{1}
        \left[
          log(f_{\pmb{X}}(\pmb{X};\pmb{\theta}_{1}))
          -
          log(f_{\pmb{X}}(\pmb{X};\pmb{\theta}_{2}))
        \right] \\
      &=\frac{1}{2}E_{1}
        \left[
          -\frac{1}{2}
            \left(
              2ln(2\pi)
              + ln(|\Gamma_{1}|)
              + ({x}-{Z\beta_{1}})^T\left[{\Gamma_{1}}\right]^{-1}({x}-{Z\beta_{1}}) 
            \right.
        \right.\\ 
      &\qquad\qquad\quad
        \left.
          \left. 
            -2ln(2\pi)
            -ln(|\Gamma_{2}|)
            -({x}-{Z\beta_{2}})^T\left[{\Gamma_{2}}\right]^{-1}({x}-{Z\beta_{2}})
          \right)
        \right] \\
      &=
        \frac
          {1}{2}
        E_{1}
        \left[
          -\frac
            {1}{2}
          \left(
            ln
            \left(
              \frac
                {|\Gamma_{1}|}
                {|\Gamma_{2}|}
              \right)
            + ({x}-{Z\beta_{1}})^T\left[{\Gamma_{1}}\right]^{-1}({x}-{Z\beta_{1}})
            - ({x}-{Z\beta_{2}})^T\left[{\Gamma_{2}}\right]^{-1}({x}-{Z\beta_{2}})
            \right)
          \right] \\
      &=
        \frac
          {1}{2}
        ln
          \left(
            \frac
              {|\Gamma_{1}|}
              {|\Gamma_{2}|}
            \right)
    \end{aligned}
  $$

***

###Bonus Question

If the **true** value of the parameter vector is $\pmb{\theta} = (\beta', \sigma^{2})'$ and the **estimated** value based on the **sample** $\widehat{\pmb{\theta}} = (\widehat{\beta'}, \widehat{\sigma^{2}})'$, one may argue that the **best** model would be one that **minimizes** the **Kullback-Leibler distance** between the joint-pdfs of **theoretical** value and the **sample** estimation, say $I(\pmb{\theta}; \widehat{\pmb{\theta}})$. Because $\pmb{\theta}$ will not be known, Hurvich and Tsai (1989) considered finding an **unbiased estimator** for $E_{\pmb{\theta}}[I(\beta, \sigma^{2}; \widehat{\beta}, \widehat{\sigma}^{2})]$, where

$$
  I(\beta, \sigma^{2}; \widehat{\beta}, \widehat{\sigma}^{2}) = \frac{1}{2}\left(\frac{\sigma^{2}}{\widehat{\sigma}^{2}} - log\left(\frac{\sigma^{2}}{\widehat{\sigma}^{2}}\right) - 1\right) + \frac{(\beta - \widehat{\beta})'Z'Z(\beta - \widehat{\beta})}{2n\widehat{\sigma}^{2}}
$$

and $\beta$ is a $k \times 1$ regression parameter vector. Show that

$$
  E_{\pmb{\theta}}[I(\beta_{1}, \sigma_{1}^{2}; \widehat{\beta}, \widehat{\sigma}^{2})] = \frac{1}{2}\left(-log(\sigma^{2}) + E_{\pmb{\theta}}[log(\widehat{\sigma}^{2})] + \frac{n+k}{n-k-2} - 1\right).
$$

**Answer:** Expectation is a linear function, so we can rewrite the above as 
$$
\begin{aligned}
  E_{\pmb{\theta}}I(\beta, \sigma^{2}; \widehat{\beta}, \widehat{\sigma}^{2}) &= 
    \frac{1}{2}E\left[\frac{\sigma^{2}}{\widehat{\sigma}^{2}} 
    - log\left(\frac{\sigma^{2}}{\widehat{\sigma}^{2}}\right) 
    - 1 + 
    \frac{(\beta - \widehat{\beta})'Z'Z(\beta - \widehat{\beta})}{n\widehat{\sigma}^{2}}\right] \\
  &=\frac{1}{2}\left(E\left[\frac{\sigma^{2}}{\widehat{\sigma}^{2}}\right] - E\left[log\left(\frac{\sigma^{2}}{\widehat{\sigma}^{2}}\right)\right] - E\left[1\right] + E\left[\frac{(\beta - \widehat{\beta})'Z'Z(\beta - \widehat{\beta})}{n\widehat{\sigma}^{2}}\right]\right)
\end{aligned}
$$

From the reference text, we are given that:
$$
\begin{aligned}
  \frac{n\widehat{\sigma}^{2}}{\sigma^{2}} &\sim \chi_{n-k}^{2} \\
  \frac{(\widehat{\beta} - \beta)'Z'Z(\widehat{\beta} - \beta)}{n\widehat{\sigma}^{2}} &\sim \chi_{k}^{2}
\end{aligned}
$$
We are also given that if $x \sim \chi_{n}^{2} \implies E[(\frac{1}{x})] = \frac{1}{n-2}$. So:
$$
\begin{aligned}
  E \left[\left(\frac{\sigma^{2}}{\widehat{\sigma}^{2}}\right)\right] &= n*\left(\frac{1}{(n-k)-2}\right) \\
  &= \frac{n}{n-k-2}
\end{aligned}
$$
In class it was shown that:
$$
  \frac{k}{n-k} \, \frac{(\widehat{\beta} - \beta)'Z'Z(\widehat{\beta} - \beta)}{n\widehat{\sigma}^{2}} \sim F_{k,n-k}
$$
Given that $E\left[F_{k,n-k}\right] = (n-k)/(n-k-2)$, we get:
$$
\begin{aligned}
  E \left[\frac{(\widehat{\beta} - \beta)'Z'Z(\widehat{\beta} - \beta)}{n\widehat{\sigma}^{2}}\right] &= \frac{k}{n-k} \, \frac{n-k}{n-k-2} \\
  &= \frac{k}{n-k-2}
\end{aligned}
$$

Taking the expectation of a scalar returns the scalar. So we can simplify the original equation as:
$$
\begin{aligned}
  E_{\pmb{\theta}}I(\beta, \sigma^{2}; \widehat{\beta}, \widehat{\sigma}^{2}) &= \frac{1}{2}\left[\frac{n}{n-k-2} - E_{\pmb{\theta}}\left[log\left(\frac{\sigma^{2}}{\widehat{\sigma}^{2}}\right)\right] - 1 + \frac{k}{n-k-2}\right] \\
  &= \frac{1}{2}\left[-E_{\pmb{\theta}}\left[log\left(\frac{\sigma^{2}}{\widehat{\sigma}^{2}}\right)\right] + \frac{n + k}{n-k-2} - 1 \right]
\end{aligned}
$$
We can rewrite the remaining log term using the fact that $log(a/b) = log(a) - log(b)$.
$$
\begin{aligned}
  E_{\pmb{\theta}}I(\beta, \sigma^{2}; \widehat{\beta}, \widehat{\sigma}^{2}) &= \frac{1}{2}\left[-\left(E_{\pmb{\theta}}\left[log\left(\sigma^{2}\right) - log\left(\widehat{\sigma}^{2}\right)\right]\right) + \frac{n + k}{n-k-2} - 1 \right] \\
  &= \frac{1}{2}\left[-E_{\pmb{\theta}}\left[log\left(\sigma^{2}\right)\right] + E_{\pmb{\theta}}\left[log\left(\widehat{\sigma}^{2}\right)\right] + \frac{n + k}{n-k-2} - 1 \right]
\end{aligned}
$$
We don't know how to evaluate $E_{\pmb{\theta}}\left[log\left(\widehat{\sigma}^{2}\right)\right]$, but $log\left(\sigma^{2}\right)$ is a constant - not a R.V - so we can rewrite the equation as:
$$
  E_{\pmb{\theta}}I(\beta, \sigma^{2}; \widehat{\beta}, \widehat{\sigma}^{2}) = \frac{1}{2}\left[-log\left(\sigma^{2}\right) + E_{\pmb{\theta}}\left[log\left(\widehat{\sigma}^{2}\right)\right] + \frac{n + k}{n-k-2} - 1 \right]
$$
as required.



***
